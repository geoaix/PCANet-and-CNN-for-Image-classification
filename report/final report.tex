\documentclass{article} % For LaTeX2e
\usepackage{mltemplate,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{amsfonts}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09






\title{PCANet and CNN for Image classification}

\setlength\belowcaptionskip{2pt}

\author{
Yin Zhang \\
Department of Statistics\\
University of Virginia\\
Charlottesville, VA 22903 \\
\texttt{yz4an@virginia.edu} \\
\And
Haoran Liu \\
Department of Computer Science \\
University of Virginia\\
Charlottesville, VA 22903  \\
\texttt{hl4fb@virginia.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

% newcommand
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\renewcommand{\b}[1] {\boldsymbol{#1}} 
\newcommand{\bb}[1] {\bar{\boldsymbol{#1}}}
\maketitle

\begin{abstract}
%Our proposal is mainly motivated by the PCANet proposed in\cite{chan2015pcanet}, where the PCA filters are learned by extracting the principal components of the image patches. The features learned by PCA filters are on par with, sometimes even better than the most state of the art algorithms. In our proposal, 

In this paper, we reimplement the PCANet proposed by \cite{chan2015pcanet} in torch7. We test the PCANet in CIFAR10 dataset for objection recognition task. We also test a variety of CNNs on these tasks to compare the performances. We find that the CNN outperform PCANet in terms of test accuracy and training time.
\end{abstract}

\section{Motivation and Overview}
This work is mainly motivated by the PCANet, which served as a simple but competitive deep learning baseline. Typically the deep learning networks (DNN) contains stacked trainable stages and is then followed by a supervised loss function. Each stage comprises a fully-connected layer or convolutional layer together followed by a nonlinear neuron function. DNN usually needs time-consuming training via back propagation.

In contrast, PCANet comprises only very basic data processing components. The multi-stage filters are learned by a simple principal component analysis, and no linear operation is involved until its very last layer, where binary hashing and histogram are employed to compute the features. Thus, back propagation is not needed for parameters updating and this results in a very efficient model training. The architecture of PCANet is shown in Figure \ref{Fig: PCA Architecture}.

We compare the performance of PCANet and DNN by testing them on CIFAR10 dataset. Our work is as follows. First, we reimplement the PCANet in torch, since the original code is written in MATLAB. Then, a 3-layer fully connected network is used to classify the features extracted by PCANet, however, only a single SVM is used in \cite{chan2015pcanet}. Third, we implement 2 CNNs and compare the performances with the PCANet. 






%\section{Previous work}
%PCANet, CNN

\begin{figure}[h] \label{Fig: PCA Architecture}
\vspace{-15pt} \centering
\includegraphics[scale=1.7,width=14cm]{pcanetstructure.png}
\vspace{-20pt}\caption{The Architecture of PCANet}
\end{figure}


\section{Models} 
Suppose that we are given $N$ input training images $\{\mathcal{I}_i\}_{i=1}^N $ of size $m \times n$, and assume that the patch size is $k_1 \times k_2$ at all stages in PCANet. The architecture of PCANet is shown in Figure \ref{Fig: PCA ArchitectureF}. Here we discard the binary hashing and histogram in the output stage and connect the remaining PCANet with regular convolution layers. The PCA filters only need to be learned from the data one time, but the convolutional filters require to be learned by back propagation. 

\subsection{The first stage of PCA Network (PCANet)}
Around each pixel, we take a $k_1 \times k_2$ patch, and we collect all (overlapping) patches of the $i$th image; i.e., $\b{x}_{i,1},...,\b{x}_{i,mn} \in \mathbb{R}^{k_1\times k_2}$. We then subtract mean from each patch and obtain $\bb{X} = [\bb{x}_{i,1},...,\bb{x}_{i,mn}] $. By constructing the same matrix for all input images and putting them together, we get
\begin{eqnarray}
	\b{X} = [\bb{X}_1,...,\bb{X}_N] \in \mathbb{R}^{k_1k_2\times Nmn}
\end{eqnarray}
In layer $1$, we can obtain the first $L_1$ PCA filters by minimizing the reconstruction error,i.e.
\begin{eqnarray}
	\b{\hat{V}} = \arg\min_{\b{V}\in \mathbb{R}^{k_1K_2\times L_1}} ||\b{X}-\b{VV}^T\b{X}||^2_F  \hspace{0.3cm} \textrm{s.t.} \hspace{0.2cm} \b{V}^T\b{V} = \b{I}
\end{eqnarray}
The solution is known as the $L_1$ principal vectors of $\b{XX}^T$. Then we can obtain the PCA filters by reshaping column $l$ of $\b{\hat{V}}$ to a matrix $\b{W}_l^1$ of dimension $k_1\times k_2$, for $l=1,...,L_1$. The $l$th filter output of the first stage is
\begin{eqnarray}
	\mathcal{I}_i^l =  \mathcal{I} * \b{W}_l^1
\end{eqnarray}
where $*$ denotes the 2D convolution. Therefore, the number of output of first stage is $L_1$, but the size of each output depends on the strides and number of zero paddings.



\subsection{The second stage of PCA Network (PCANet)}
Almost repeating the same process as the first stage. We can collect all the overlapping patches $\mathcal{I}_i^l$ from the first stage, and form $\bb{Y}^l = [\bb{y}_{i,l,1},...,\bb{y}_{i,l,mn}]  \in \mathbb{R}^{k_1K_2\times mn}$, where $\bb{y}_{i,l,j}$ is the $j$th mean-removed patch in $\mathcal{I}_i^l$. We further define $\b{Y}^l = [\bb{Y}_1^l,...,\bb{Y}_N^l]$ for the matrix collecting all the mean-removed patches of the $l$th filter, and concatenate $\b{Y}^l$ for all the filter outputs as 
\begin{eqnarray}
	\b{Y} = [\b{Y}^1,...,\b{Y}^{L_1}] \in \mathbb{R}^{k_1k_2\times L_1Nmn}
\end{eqnarray} 
The PCA filters of second stage are the eigenvectors of matrix $\b{YY}^T$. The number of outputs of the second stage is $L_1L_2$. 

\subsection{Output Stage} \label{subsection: Output Stage}
For each of the $L_1$ input images $\mathcal{I}_i^l$ for the second stage, it has $L_2$ real-valued outputs $ \{\mathcal{I}_i^l * \b{W}_l^2 \}$ from the second stage. We binarize the outputs and get $\{H(\mathcal{I}_i^l * \b{W}_l^2) \}$, where $H(x)=1$, if $x>0$ and $H(x)=0$ otherwise. Then we convert the $L_2$ outputs into a single integer-valued ``image":
\begin{eqnarray*}
	\mathcal{T}_i^l=\sum_{i=1}^{L_2} H(\mathcal{I}_i^l * \b{W}_l^2)
\end{eqnarray*} 
For each of the $L_1$ images, we partition it into $B$ blocks. Then we compute the histogram (with 32 bins, in \cite{chan2015pcanet}, they use $2^{L_2}$ bins) of the decimal values in each block, and concatenate all the B histograms into one vector. Thus the output vector has dimension $32 \times L_1 \times B$, if there is no overlap between the blocks. In \cite{chan2015pcanet}, this dimension is $2^{L_2} \times L_1 \times B$.

\section{Experiments}
We evaluate the PCANet and CNNs in various tasks, and compare the results with the state of the art and the baselines in \cite{chan2015pcanet}. The code is released at \url{https://github.com/tonyzhang1231/PCANet-CNN-torch}. Unfortunately, we haven't developed the GPU mode for this program, and only CPU mode is available at this moment.

For each task, the patch size of PCANet is fixed to $7 \times 7$, the stride is $1$ in both height and width direction, and no zero padding is imposed. The number of PCANet stages is $2$, and each stage has $8$ filters ($L_1=L_2=8$). In the output stage, the block size of histogram is $8 \times 6$, and the block overlap ratio is $0.5$. The PCA filters in stage 1 are displayed in Figure \ref{Fig: filters} (top).

For the fully connected networks applied to the PCA features. The filters are randomly initialized. The batch size is $64$. The learning rate is initialized at $0.001$, and it will decay with a factor of $0.9$ every $5$ epochs. The momentum is $0.9$. The max number of epochs is $100$.

For the CNN1, there are 2 convolutional layers, followed by 2 linear layers and a softmax layer. The first convolutional layer has 8 filters and the second has 16 filters, each of them is followed by a RELU and dropout layer. The first linear layer has dimension $400 \times 120$ and the second has dimension $120 \times 10$. The loss function is cross entropy. The patch size is $5 \times 5$, The batchSize is $64$. The learning rate is initialized at $0.001$, and it will decay with a factor of $0.9$ every $5$ epochs. The momentum is $0.9$. The max number of epochs is $100$. The convolutional filters in the first layer are shown in Figure \ref{Fig: filters} (bottom).

For CNN2, there are 3 convolutional layers, followed by 3 linear layers and a softmax layer. The first convolutional layer has 32 filters and the second has 64 filters, each of them is followed by a RELU and dropout layer. The third convolutional layer is a $1\times 1$ convolutional layer. The first linear layer has dimension $400 \times 1024$, the second has dimension $1024 \times 128$ and the third has dimension $128 \times 10$, each is preceded with a dropout layer with $p=0.5$ and a RELU layer. The loss function is cross entropy. The patch size is $5 \times 5$, The batchSize is $64$. The learning rate is initialized at $0.001$, and it will decay with a factor of $0.9$ every $5$ epochs. The momentum is $0.9$. The max number of epochs is $100$. 

All the programs are run on a single CPU.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.7,width=15cm]{PCA_filters.png} \label{Fig: PCA}
		\includegraphics[scale=1.7,width=15cm]{CNN_filters.png} \label{Fig: CNN1}
	\end{center}
	\caption{The PCA filters (top) and CNN filters (bottom)}\label{Fig: filters}
\end{figure}


\subsection{Object Recognition on CIFAR10}
For this dataset, we random select $5000$ images from training data as the validation data, and the rest $45000$ images are used as training data. We do not change the test data, which contains $10000$ images. For PCANet, we do not do any preprocessing. For CNN, we normalize each channel by subtracting the mean from each channel and divide it by its standard deviation. 

\begin{table}[h]
	\begin{center}
		\begin{tabular}{|c|c|}
		\hline
		Model & test accuracy  \\
		\hline
		PCANet + Linear & $59.06$ \\
		\hline
		CNN-1 & $65.02$ \\ 	
		\hline
		CNN-2 & $70.82$ \\ 	
		\hline
		PCANet in \cite{chan2015pcanet} & $77.14$\\
			\hline
		\end{tabular}
	\end{center}
\end{table}



%\subsection{Digit recognition on MNIST}
%
%\subsection{Face Verification on LFW}


\section{Discussion}
In \cite{chan2015pcanet}, the output of the PCANet is $2^{L_2}L_1B$. If $L_1=L_2=8$ and $B=48$, the length of output is $98304$, which is much larger than the original image, which is $3\times 32 \times 32=3072$. This will cause the memory problem, because the PCA features are estimated to be more than $20$ gigabytes, and are not able to be fit in the memory. So in our work, we reduce the dimension of output vector to $3840$ by setting the number of bins to $32$ in section \ref{subsection: Output Stage}. This may explain the decrease in the performance compared with the result in \cite{chan2015pcanet}. 

The training time of PCA filter took around $3$ hours and the PCA features took another $3$ hours. The FC net took $2$ hours to go through $100$ epochs. Therefore, the total time is $8$ hours. However, the CNN-1 with $2$ layers only took less than $1$ hour to achieve a higher accuracy of $65.02\%$ and the CNN-2 only took $1.5$ hour to achieve a even higher accuracy of $70.82\%$.

We tried $500$ epochs for both the CNNs and PCANet, but the validation results do not change much after $50$ epochs, with the minimal learning rate $0.00001$. The validation accuracy of PCANet and CNNs are shown in Figure \ref{Fig: validation result}. As we can see the convergence of PCANet is faster than the CNNs. This is primarily because the PCANet already learned a good initial representation for the images, so the linear layers can converge in fewer iterations. The training accuracy approaches to $1$ as the number of epochs increases, although not presented here. 

Hence, we did not see too many advantages of the PCANet.

\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=1.7,width=10cm]{PCANet_val_list.png} 
		\includegraphics[scale=1.7,width=10cm]{CNN_val_list.png}
		\includegraphics[scale=1.7,width=10cm]{DCNN_val_list.png}
	\end{center}
	\caption{The validation results (top) for PCANet, CNN-1 (middle) and CNN-2 (bottom)}\label{Fig: validation result}
\end{figure}

\section{Future work}
To develop a nn.Module class for PCANet, which inherit the basic nn.Module class. Then it can be integrated into a larger network. The problem we can see so far is that the PCANet can not be placed in the middle of the network. Since if the input of the PCA layers change, we need to retrain the PCA filters, which is super computationally expensive.

\bibliography{mybib}{}
\bibliographystyle{ieeetr}
\end{document}
